{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendations (Neighbor-Based Collaborative Filtering)\n",
    "\n",
    "![alt text](amazon_prime.png \"Movie Recommendations (source https://www.amazon.com)\")\n",
    "\n",
    "In this notebook, we will have a look at the [MovieLens](https://grouplens.org/datasets/movielens/) dataset, which is a popular dataset for building and benchmarking recommender systems. The dataset version we work with is the 1M dataset, which contains 1,000,209 ratings for about 3,900 movies made by 6,040 users in the year 2000.\n",
    "\n",
    "In order to build a recommender system based on neighbor-based collaborative filtering, we will make use of the [Surpise](https://surprise.readthedocs.io/en/stable/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings dataframe:\n",
      "   user_id  movie_id  rating\n",
      "0        1      1193       5\n",
      "1        1       661       3\n",
      "2        1       914       3\n",
      "3        1      3408       4\n",
      "4        1      2355       5\n",
      "5        1      1197       3\n",
      "6        1      1287       5\n",
      "7        1      2804       5\n",
      "8        1       594       4\n",
      "9        1       919       4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# parse all the data\n",
    "movies = pd.read_csv('movies.csv', \n",
    "                     sep='\\t', \n",
    "                     encoding='latin-1', \n",
    "                     usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "users = pd.read_csv('users.csv', \n",
    "                    sep='\\t', \n",
    "                    encoding='latin-1', \n",
    "                    usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv', \n",
    "                      sep='\\t', \n",
    "                      encoding='latin-1', \n",
    "                      usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# print the first 10 rows\n",
    "print(\"The ratings dataframe:\")\n",
    "print(ratings.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same of demonstration, we only consider a subset of the dataset (otherwise, training and testing the model takes much longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the rankings dataframe (random_state to get the same sequence of \n",
    "# random elements each time this cell is executed)\n",
    "small_ratings = ratings.sample(frac=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Dataset (Surpise library) via the DataFrame (Pandas library)\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# required order: user id, item id, and rating\n",
    "data = Dataset.load_from_df(small_ratings[['user_id', 'movie_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data into training and test set. The **test set must not be touched during training** (in order to obtain a realistic estimate for the performance of the model on new, unseen data). In case model parameters have also to be tuned, one has to make use of an **additional validation set** (i.e., the training set has to be split up). Note that you can also use the **automatic cross-validation procedure** provided by the Surprise library for that, see the [GridSearchCV](https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv) example (not used here though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# use 25% of the data as test set (random subsets!)\n",
    "# random set: same random subsets each time this cell is executed\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate a KNNBasic class, see [https://surprise.readthedocs.io/en/stable/knn_inspired.html](https://surprise.readthedocs.io/en/stable/knn_inspired.html) for details. Afterwards, we fit the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic, KNNWithMeans\n",
    "\n",
    "# use user-based centered cosine similarity (pearson)\n",
    "# k=10 nearest neighbors; at least min_k=1 nearest neighbor\n",
    "# has to be given in the set; otherwise, one falls back to \n",
    "# global mean ratings\n",
    "sim_options = {\n",
    "    \"name\": \"pearson\",\n",
    "    \"user_based\": True, \n",
    "}\n",
    "algo = KNNWithMeans(k=40, min_k=1, sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNWithMeans at 0x7f7b6c44bb50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on the training set\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute predictions for the hold-out test set. **This set should only be used at the very end.** That is, model selection (e.g., selecting a good assignment for k or trying out other models) has to be done on the training data only! Afterwards, we can compute the RMSE to assess the quality of the model on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1003347703592203"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 655        item: 1235       r_ui = 4.00   est = 2.52   {'actual_k': 0, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# take first test instance\n",
    "user_id, movie_id, r_ui = testset[0]\n",
    "\n",
    "# get a prediction for specific user and item\n",
    "pred = algo.predict(user_id, movie_id, r_ui=r_ui, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
