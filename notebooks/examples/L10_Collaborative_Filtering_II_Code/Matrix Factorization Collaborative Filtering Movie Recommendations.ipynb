{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendations (Matrix Factorization)\n",
    "\n",
    "![alt text](amazon_prime.png \"Movie Recommendations (source https://www.amazon.com)\")\n",
    "\n",
    "In this notebook, we will have a look at the [MovieLens](https://grouplens.org/datasets/movielens/) dataset, which is a popular dataset for building and benchmarking recommender systems. The dataset version we work with is the 1M dataset, which contains 1,000,209 ratings for about 3,900 movies made by 6,040 users in the year 2000.\n",
    "\n",
    "In order to build a recommender system based on matrix factorization filtering, we will make use of the [Surpise](https://surprise.readthedocs.io/en/stable/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings dataframe:\n",
      "   user_id  movie_id  rating\n",
      "0        1      1193       5\n",
      "1        1       661       3\n",
      "2        1       914       3\n",
      "3        1      3408       4\n",
      "4        1      2355       5\n",
      "5        1      1197       3\n",
      "6        1      1287       5\n",
      "7        1      2804       5\n",
      "8        1       594       4\n",
      "9        1       919       4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# parse all the data\n",
    "movies = pd.read_csv('movies.csv', \n",
    "                     sep='\\t', \n",
    "                     encoding='latin-1', \n",
    "                     usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "users = pd.read_csv('users.csv', \n",
    "                    sep='\\t', \n",
    "                    encoding='latin-1', \n",
    "                    usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv', \n",
    "                      sep='\\t', \n",
    "                      encoding='latin-1', \n",
    "                      usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# print the first 10 rows\n",
    "print(\"The ratings dataframe:\")\n",
    "print(ratings.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same of demonstration, we only consider a subset of the dataset (otherwise, training and testing the model takes much longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the rankings dataframe (random_state to get the same sequence of \n",
    "# random elements each time this cell is executed)\n",
    "small_ratings = ratings.sample(frac=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Dataset (Surpise library) via the DataFrame (Pandas library)\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# required order: user id, item id, and rating\n",
    "data = Dataset.load_from_df(small_ratings[['user_id', 'movie_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data into training and test set. The **test set must not be touched during training** (in order to obtain a realistic estimate for the performance of the model on new, unseen data). In case model parameters have also to be tuned, one has to make use of an **additional validation set** (i.e., the training set has to be split up). Note that you can also use the **automatic cross-validation procedure** provided by the Surprise library for that, see the [GridSearchCV](https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv) example (not used here though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# use 25% of the data as test set (random subsets!)\n",
    "# random set: same random subsets each time this cell is executed\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate a matrix factorization model, see [https://surprise.readthedocs.io/en/stable/matrix_factorization.html#](https://surprise.readthedocs.io/en/stable/matrix_factorization.html). Afterwards, we fit the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "algo = SVD(n_factors=100, n_epochs=50, biased=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f1dfcd29b50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on the training set\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute predictions for the hold-out test set. **This set should only be used at the very end.** That is, model selection (e.g., selecting a good assignment for k or trying out other models) has to be done on the training data only! Afterwards, we can compute the RMSE to assess the quality of the model on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9720503252199726"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 655        item: 1235       r_ui = 4.00   est = 3.21   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# take first test instance\n",
    "user_id, movie_id, r_ui = testset[0]\n",
    "\n",
    "# get a prediction for specific user and item\n",
    "pred = algo.predict(user_id, movie_id, r_ui=r_ui, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
