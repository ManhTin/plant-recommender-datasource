{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b27dba",
   "metadata": {},
   "source": [
    "This notebook demonstrates the basic behind gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329a6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def gradient_descent(func, grad_func, w_init, n_epochs=100, lr=0.001, verbose=0):\n",
    "    \n",
    "    i = 0\n",
    "    w = w_init\n",
    "\n",
    "    # conduct a fixed number of steps; other stopping criteria\n",
    "    # could also be used (e.g., stopping once the difference\n",
    "    # between the current function value and the one of the \n",
    "    # previous iteration becomes very small)\n",
    "    while i < n_epochs:\n",
    "        \n",
    "        # conduct gradient update step!\n",
    "        delta_w = -lr * grad_func(w)\n",
    "        w = w + delta_w\n",
    "                \n",
    "        if verbose > 0:\n",
    "            print(\"f={}; w: {}\".format(func(w), w))\n",
    "            \n",
    "        # increment counter\n",
    "        i += 1            \n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b8cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function (e.g., f(w_1,w_2) = w_1*w_1 + w_2*w_2) for d=2)\n",
    "def f(w):\n",
    "    return numpy.sum(w*w)\n",
    "\n",
    "# corresponding gradient\n",
    "def grad(w):\n",
    "    return 2*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c91586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f=128.0; w: [8. 8.]\n",
      "f=81.92000000000002; w: [6.4 6.4]\n",
      "f=52.4288; w: [5.12 5.12]\n",
      "f=33.554432; w: [4.096 4.096]\n",
      "f=21.47483648; w: [3.2768 3.2768]\n",
      "f=13.743895347200002; w: [2.62144 2.62144]\n",
      "f=8.796093022208003; w: [2.097152 2.097152]\n",
      "f=5.629499534213123; w: [1.6777216 1.6777216]\n",
      "f=3.602879701896398; w: [1.34217728 1.34217728]\n",
      "f=2.305843009213695; w: [1.07374182 1.07374182]\n",
      "f=1.475739525896765; w: [0.85899346 0.85899346]\n",
      "f=0.9444732965739295; w: [0.68719477 0.68719477]\n",
      "f=0.6044629098073148; w: [0.54975581 0.54975581]\n",
      "f=0.38685626227668146; w: [0.43980465 0.43980465]\n",
      "f=0.24758800785707613; w: [0.35184372 0.35184372]\n",
      "f=0.1584563250285287; w: [0.28147498 0.28147498]\n",
      "f=0.10141204801825837; w: [0.22517998 0.22517998]\n",
      "f=0.06490371073168535; w: [0.18014399 0.18014399]\n",
      "f=0.041538374868278626; w: [0.14411519 0.14411519]\n",
      "f=0.026584559915698323; w: [0.11529215 0.11529215]\n",
      "f=0.017014118346046925; w: [0.09223372 0.09223372]\n",
      "f=0.010889035741470033; w: [0.07378698 0.07378698]\n",
      "f=0.00696898287454082; w: [0.05902958 0.05902958]\n",
      "f=0.004460149039706126; w: [0.04722366 0.04722366]\n",
      "f=0.0028544953854119206; w: [0.03777893 0.03777893]\n"
     ]
    }
   ],
   "source": [
    "# starting point\n",
    "w_init = numpy.array([10,10])\n",
    "\n",
    "# learning rate (usually has a big impact!)\n",
    "lr = 0.1\n",
    "\n",
    "# apply gradient descent\n",
    "w_opt = gradient_descent(f, grad, w_init, n_epochs=25, lr=lr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a448f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
